{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# **Spark + Python = PySpark**\n",
    "\n",
    "#### Esse notebook introduz os conceitos básicos do Spark através de sua interface com a linguagem Python. Como aplicação inicial faremos o clássico examplo de contador de palavras . Com esse exemplo é possível entender a lógica de programação funcional para as diversas tarefas de exploração de dados distribuídos.\n",
    "\n",
    "#### Para isso utilizaremos o livro texto [Trabalhos completos de William Shakespeare](http://www.gutenberg.org/ebooks/100) obtidos do  [Projeto Gutenberg](http://www.gutenberg.org/wiki/Main_Page).  Veremos que esse mesmo algoritmo pode ser empregado em textos de qualquer tamanho.\n",
    "\n",
    "#### ** Esse notebook contém:  **\n",
    "#### *Parte 1:* Criando uma base RDD e RDDs de tuplas\n",
    "#### *Parte 2:* Manipulando RDDs de tuplas\n",
    "#### *Parte 3:* Encontrando palavras únicas e calculando médias\n",
    "#### *Parte 4:* Aplicar contagem de palavras em um arquivo\n",
    "#### *Parte 5:* Similaridade entre Objetos\n",
    "#### Para os exercícios é aconselhável consultar a documentação da [API do PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Criando e Manipulando RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos criar uma base RDD a partir de uma lista com o comando `parallelize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) Criando uma base RDD **\n",
    "#### Podemos criar uma base RDD de diversos tipos e fonte do Python com o comando `sc.parallelize(fonte, particoes)`, sendo fonte uma variável contendo os dados (ex.: uma lista) e particoes o número de partições para trabalhar em paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['gato', 'elefante', 'rato', 'rato', 'gato']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "ListaPalavras = ['gato', 'elefante', 'rato', 'rato', 'gato']\n",
    "palavrasRDD = sc.parallelize(ListaPalavras, 4)\n",
    "print type(palavrasRDD)\n",
    "print palavrasRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Plural **\n",
    "\n",
    "#### Vamos criar uma função que transforma uma palavra no plural adicionando uma letra 's' ao final da string. Em seguida vamos utilizar a função `map()` para aplicar a transformação em cada palavra do RDD.\n",
    "\n",
    "#### Em Python (e muitas outras linguagens) a concatenação de strings é custosa. Uma alternativa melhor é criar uma nova string utilizando [`str.format()`](https://docs.python.org/2/library/string.html#format-string-syntax).\n",
    "\n",
    "#### Nota: a string entre os conjuntos de três aspas representa a documentação da função. Essa documentação é exibida com o comando `help()`. Vamos utilizar a padronização de documentação sugerida para o Python, manteremos essa documentação em inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gatos\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def Plural(palavra):\n",
    "    \"\"\"Adds an 's' to `palavra`.\n",
    "\n",
    "    Args:\n",
    "        palavra (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return ('{}s').format(palavra)\n",
    "\n",
    "print Plural('gato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Plural in module __main__:\n",
      "\n",
      "Plural(palavra)\n",
      "    Adds an 's' to `palavra`.\n",
      "    \n",
      "    Args:\n",
      "        palavra (str): A string.\n",
      "    \n",
      "    Returns:\n",
      "        str: A string with 's' added to it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert Plural('rato')=='ratos', 'resultado incorreto!'\n",
    "print 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1c) Aplicando a função ao RDD **\n",
    "#### Transforme cada palavra do nosso RDD em plural usando [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) \n",
    "\n",
    "#### Em seguida, utilizaremos o comando  [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) que retorna a RDD como uma lista do Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralRDD = palavrasRDD.map(Plural)\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
    "print 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Nota: ** utilize o comando `collect()` apenas quando tiver certeza de que a lista caberá na memória. Para gravar os resultados de volta em arquivo texto ou base de dados utilizaremos outro comando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1d) Utilizando uma função `lambda` **\n",
    "#### Repita a criação de um RDD de plurais, porém utilizando uma função lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralLambdaRDD = palavrasRDD.map(lambda x: Plural(x))\n",
    "print pluralLambdaRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralLambdaRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
    "print 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1e) Tamanho de cada palavra **\n",
    "#### Agora use `map()` e uma função `lambda` para retornar o número de caracteres em cada palavra. Utilize `collect()` para armazenar o resultado em forma de listas na variável destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralTamanho = (pluralRDD\n",
    "                 .map(lambda x: len(x))\n",
    "                 .collect()\n",
    "                 )\n",
    "print pluralTamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralTamanho==[5,9,5,5,5], 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1f) RDDs de pares e tuplas **\n",
    "\n",
    "#### Para contar a frequência de cada palavra de maneira distribuída, primeiro devemos atribuir um valor para cada palavra do RDD. Isso irá gerar um base de dados (chave, valor). Desse modo podemos agrupar a base através da chave, calculando a soma dos valores atribuídos. No nosso caso, vamos atribuir o valor `1` para cada palavra.\n",
    "\n",
    "#### Um RDD contendo a estrutura de tupla chave-valor `(k,v) ` é chamada de RDD de tuplas ou *pair RDD*.\n",
    "\n",
    "#### Vamos criar nosso RDD de pares usando a transformação  `map()` com uma função `lambda()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gato', 1), ('elefante', 1), ('rato', 1), ('rato', 1), ('gato', 1)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavraPar = palavrasRDD.map(lambda x: (x, 1))\n",
    "print palavraPar.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert palavraPar.collect() == [('gato',1),('elefante',1),('rato',1),('rato',1),('gato',1)], 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 2: Manipulando RDD de tuplas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos manipular nossa RDD para contar as palavras do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Função `groupByKey()`  **\n",
    "\n",
    "#### A função [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) agrupa todos os valores de um RDD através da chave (primeiro elemento da tupla) agregando os valores em uma lista.\n",
    "\n",
    "#### Essa abordagem tem um ponto fraco pois:\n",
    "  + #### A operação requer que os dados distribuídos sejam movidos em massa para que permaneçam na partição correta.\n",
    "  + #### As listas podem se tornar muito grandes. Imagine contar todas as palavras do Wikipedia: termos comuns como \"a\", \"e\" formarão uma lista enorme de valores que pode não caber na memória do processo escravo.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rato: [1, 1]\n",
      "elefante: [1]\n",
      "gato: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasGrupo = palavraPar.groupByKey()\n",
    "for chave, valor in palavrasGrupo.collect():\n",
    "    print '{0}: {1}'.format(chave, list(valor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(palavrasGrupo.mapValues(lambda x: list(x)).collect()) == [('elefante', [1]), ('gato',[1, 1]), ('rato',[1, 1])],'Valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Calculando as contagens **\n",
    "#### Após o  `groupByKey()` nossa RDD contém elementos compostos da palavra, como chave, e um iterador contendo todos os valores correspondentes aquela chave.\n",
    "#### Utilizando a transformação `mapValues()` e a função `sum()`, contrua um novo RDD que consiste de tuplas (chave, soma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rato', 2), ('elefante', 1), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemGroup = palavrasGrupo.mapValues(lambda x: sum(x))\n",
    "print contagemGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert list(sorted(contagemGroup.collect()))==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) `reduceByKey` **\n",
    "#### Um comando mais interessante para a contagem é o  [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) que cria uma nova RDD de tuplas.\n",
    "\n",
    "#### Essa transformação aplica a transformação `reduce()` vista na aula anterior para os valores de cada chave. Dessa forma, a função de transformação pode ser aplicada em cada partição local para depois ser enviada para redistribuição de partições, reduzindo o total de dados sendo movidos e não mantendo listas grandes na memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rato', 2), ('elefante', 1), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagem = palavraPar.reduceByKey(lambda x,y: x+y)\n",
    "print contagem.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagem.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2d) Agrupando os comandos **\n",
    "\n",
    "#### A forma mais usual de realizar essa tarefa, partindo do nosso RDD palavrasRDD, é encadear os comandos map e reduceByKey em uma linha de comando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rato', 2), ('elefante', 1), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemFinal = (palavrasRDD\n",
    "                 .map(lambda x: (x, 1))\n",
    "                 .reduceByKey(lambda x,y: x+y)\n",
    "                 )\n",
    "print contagemFinal.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagemFinal.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 3: Encontrando as palavras únicas e calculando a média de contagem **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Palavras Únicas **\n",
    "\n",
    "#### Calcule a quantidade de palavras únicas do RDD. Utilize comandos de RDD da API do PySpark e alguma das últimas RDDs geradas nos exercícios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasUnicas = (palavrasRDD\n",
    "                  .map(lambda x: (x, 1))\n",
    "                  .reduceByKey(lambda x,y: x+y)\n",
    "                  .count()\n",
    "                 )\n",
    "print palavrasUnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert palavrasUnicas==3, 'valor incorreto!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Calculando a Média de contagem de palavras **\n",
    "\n",
    "#### Encontre a média de frequência das palavras utilizando o RDD `contagem`.\n",
    "\n",
    "#### Note que a função do comando `reduce()` é aplicada em cada tupla do RDD. Para realizar a soma das contagens, primeiro é necessário mapear o RDD para um RDD contendo apenas os valores das frequências (sem as chaves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.67\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# add é equivalente a lambda x,y: x+y\n",
    "from operator import add\n",
    "total = (contagemFinal\n",
    "         .map(lambda (x,y): y)\n",
    "         .reduce(add)\n",
    "         )\n",
    "media = total / float(palavrasUnicas)\n",
    "print total\n",
    "print round(media, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert round(media, 2)==1.67, 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 4: Aplicar nosso algoritmo em um arquivo **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Função `contaPalavras` **\n",
    "\n",
    "#### Para podermos aplicar nosso algoritmo genéricamente em diversos RDDs, vamos primeiro criar uma função para aplicá-lo em qualquer fonte de dados. Essa função recebe de entrada um RDD contendo uma lista de chaves (palavras) e retorna um RDD de tuplas com as chaves e a contagem delas nessa RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rato', 2), ('elefante', 1), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def contaPalavras(chavesRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        chavesRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return (chavesRDD\n",
    "            .map(lambda x: (x, 1))\n",
    "            .reduceByKey(lambda x,y: x+y)\n",
    "           )\n",
    "\n",
    "print contaPalavras(palavrasRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contaPalavras(palavrasRDD).collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4b) Normalizando o texto **\n",
    "\n",
    "#### Quando trabalhamos com dados reais, geralmente precisamos padronizar os atributos de tal forma que diferenças sutis por conta de erro de medição ou diferença de normatização, sejam desconsideradas. Para o próximo passo vamos padronizar o texto para:\n",
    "  + #### Padronizar a capitalização das palavras (tudo maiúsculo ou tudo minúsculo).\n",
    "  + #### Remover pontuação.\n",
    "  + #### Remover espaços no início e no final da palavra.\n",
    " \n",
    "#### Crie uma função `removerPontuacao` que converte todo o texto para minúscula, remove qualquer pontuação e espaços em branco no início ou final da palavra. Para isso, utilize a biblioteca [re](https://docs.python.org/2/library/re.html) para remover todo texto que não seja letra, número ou espaço, encadeando com as funções de string para remover espaços em branco e converter para minúscula (veja [Strings](https://docs.python.org/2/library/stdtypes.html?highlight=str.lower#string-methods))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola quem esta ai\n",
      "sem espaco esublinhado\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "def removerPontuacao(texto):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        texto (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower()\n",
    "print removerPontuacao('Ola, quem esta ai??!')\n",
    "print removerPontuacao(' Sem espaco e_sublinhado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert removerPontuacao(' O uso de virgulas, embora permitido, nao deve contar. ')=='o uso de virgulas embora permitido nao deve contar', 'string incorreta!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4c) Carregando arquivo texto  **\n",
    "\n",
    "#### Para a próxima parte vamos utilizar o livro [Trabalhos completos de William Shakespeare](http://www.gutenberg.org/ebooks/100) do [Projeto Gutenberg](http://www.gutenberg.org/wiki/Main_Page). \n",
    "\n",
    "#### Para converter um texto em uma RDD, utilizamos a função `textFile()` que recebe como entrada o nome do arquivo texto que queremos utilizar e o número de partições.\n",
    "\n",
    "#### O nome do arquivo texto pode se referir a um arquivo local ou uma URI de arquivo distribuído (ex.: hdfs://).\n",
    "\n",
    "#### Vamos também aplicar a função `removerPontuacao()` para normalizar o texto e verificar as 15 primeiras linhas com o comando `take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existe!\n",
      "0: the project gutenberg ebook of the complete works of william shakespeare by\n",
      "1: william shakespeare\n",
      "2: \n",
      "3: this ebook is for the use of anyone anywhere at no cost and with\n",
      "4: almost no restrictions whatsoever  you may copy it give it away or\n",
      "5: reuse it under the terms of the project gutenberg license included\n",
      "6: with this ebook or online at wwwgutenbergorg\n",
      "7: \n",
      "8: this is a copyrighted project gutenberg ebook details below\n",
      "9: please follow the copyright guidelines in this file\n",
      "10: \n",
      "11: title the complete works of william shakespeare\n",
      "12: \n",
      "13: author william shakespeare\n",
      "14: \n"
     ]
    }
   ],
   "source": [
    "# Apenas execute a célula\n",
    "import os.path\n",
    "import urllib2\n",
    "\n",
    "url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt' # url do livro\n",
    "\n",
    "arquivo = os.path.join('Data','Aula02','shakespeare.txt') # local de destino: 'Data/Aula02/shakespeare.txt'\n",
    "\n",
    "if os.path.isfile(arquivo):     # verifica se já fizemos download do arquivo\n",
    "    print 'Arquivo já existe!'\n",
    "else:\n",
    "    try:        \n",
    "        response = urllib2.urlopen(url)\n",
    "        arquivo = (response.read()).split() #ja gera uma lista de palavras\n",
    "    except IOError:\n",
    "        print 'Impossível fazer o download: {0}'.format(url)\n",
    "\n",
    "# lê o arquivo com textFile e aplica a função removerPontuacao        \n",
    "shakesRDD = (sc\n",
    "             .textFile(arquivo, 8)\n",
    "             .map(removerPontuacao)\n",
    "             )\n",
    "\n",
    "# zipWithIndex gera tuplas (conteudo, indice) onde indice é a posição do conteudo na lista sequencial\n",
    "# Ex.: sc.parallelize(['gato','cachorro','boi']).zipWithIndex() ==> [('gato',0), ('cachorro',1), ('boi',2)]\n",
    "# sep.join() junta as strings de uma lista através do separador sep. Ex.: ','.join(['a','b','c']) ==> 'a,b,c'\n",
    "print '\\n'.join(shakesRDD\n",
    "                .zipWithIndex()\n",
    "                .map(lambda (linha, num): '{0}: {1}'.format(num,linha))\n",
    "                .take(15)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4d) Extraindo as palavras **\n",
    "#### Antes de poder usar nossa função Before we can use the `contaPalavras()`, temos ainda que trabalhar em cima da nossa RDD:\n",
    "  + #### Precisamos gerar listas de palavras ao invés de listas de sentenças.\n",
    "  + #### Eliminar linhas vazias.\n",
    " \n",
    "#### As strings em Python tem o método [split()](https://docs.python.org/2/library/string.html#string.split) que faz a separação de uma string por separador. No nosso caso, queremos separar as strings por espaço. \n",
    "\n",
    "#### Utilize a função `map()` para gerar um novo RDD como uma lista de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'the', u'project', u'gutenberg', u'ebook', u'of', u'the', u'complete', u'works', u'of', u'william', u'shakespeare', u'by'], [u'william', u'shakespeare'], [], [u'this', u'ebook', u'is', u'for', u'the', u'use', u'of', u'anyone', u'anywhere', u'at', u'no', u'cost', u'and', u'with'], [u'almost', u'no', u'restrictions', u'whatsoever', u'you', u'may', u'copy', u'it', u'give', u'it', u'away', u'or']]\n",
      "124787\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.map(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print shakesPalavrasRDD.take(5)\n",
    "print total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conforme deve ter percebido, o uso da função `map()` gera uma lista para cada linha, criando um RDD contendo uma lista de listas.\n",
    "\n",
    "#### Para resolver esse problema, o Spark possui uma função análoga chamada `flatMap()` que aplica a transformação do `map()`, porém *achatando* o retorno em forma de lista para uma lista unidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']\n",
      "903705\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print shakesPalavrasRDD.top(5)\n",
    "print total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** os asserts abaixo de contagem de palavra podem falhar por diferença de formato do arquivo .txt antigo e novo. Eu avaliarei somente os códigos nesse trecho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valor incorreto de palavras!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-76702fed04e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m927631\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m928908\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"valor incorreto de palavras!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mshakesPalavrasRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'zwaggerd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'zounds'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'zounds'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'zounds'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'zounds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lista incorreta de palavras'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valor incorreto de palavras!"
     ]
    }
   ],
   "source": [
    "assert total==927631 or total == 928908, \"valor incorreto de palavras!\"\n",
    "print \"OK\"\n",
    "assert shakesPalavrasRDD.top(5)==[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds'],'lista incorreta de palavras'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4e) Remover linhas vazias **\n",
    "\n",
    "#### Para o próximo passo vamos filtrar as linhas vazias com o comando `filter()`. Uma linha vazia é uma string sem nenhum conteúdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903705\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "shakesLimpoRDD = shakesPalavrasRDD.filter(lambda x: x != ' ')\n",
    "total = shakesLimpoRDD.count()\n",
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valor incorreto!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-eaded5957490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m882996\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valor incorreto!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valor incorreto!"
     ]
    }
   ],
   "source": [
    "assert total==882996, 'valor incorreto!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4f) Contagem de palavras **\n",
    "#### Agora que nossa RDD contém uma lista de palavras, podemos aplicar nossa função `contaPalavras()`.\n",
    "\n",
    "#### Aplique a função em nossa RDD e utilize a função `takeOrdered` para imprimir as 15 palavras mais frequentes.\n",
    "\n",
    "#### `takeOrdered()` pode receber um segundo parâmetro que instrui o Spark em como ordenar os elementos. Ex.:\n",
    "\n",
    "#### `takeOrdered(15, key=lambda x: -x)`: ordem decrescente dos valores de x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 27825\n",
      "and: 26791\n",
      "i: 20681\n",
      "to: 19261\n",
      "of: 18289\n",
      "a: 14667\n",
      "you: 13716\n",
      "my: 12481\n",
      "that: 11135\n",
      "in: 11027\n",
      "is: 9621\n",
      "not: 8745\n",
      "for: 8261\n",
      "with: 8046\n",
      "me: 7769\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "top15 = contaPalavras(shakesLimpoRDD).takeOrdered(15, key = lambda (x, y): -y)\n",
    "print '\\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valores incorretos!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-6bbde664e0d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m assert top15 == [(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463),\n\u001b[0;32m      2\u001b[0m                    \u001b[1;33m(\u001b[0m\u001b[1;34mu'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14593\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu'you'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13615\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu'my'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12481\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu'in'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10956\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu'that'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10890\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                    (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)],'valores incorretos!'\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valores incorretos!"
     ]
    }
   ],
   "source": [
    "assert top15 == [(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463),\n",
    "                   (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890),\n",
    "                   (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)],'valores incorretos!'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 5: Similaridade entre Objetos **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nessa parte do laboratório vamos aprender a calcular a distância entre atributos numéricos, categóricos e textuais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5a) Vetores no espaço Euclidiano **\n",
    "\n",
    "#### Quando nossos objetos são representados no espaço Euclidiano, medimos a similaridade entre eles através da *p-Norma* definida por:\n",
    "\n",
    "#### $$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{1/p}$$\n",
    "\n",
    "#### As normas mais utilizadas são $p=1,2,\\infty$ que se reduzem em distância absoluta, Euclidiana e máxima distância:\n",
    "\n",
    "#### $$d(x,y,1) = \\sum_{i=1}^{n}{|x_i - y_i|}$$\n",
    "\n",
    "#### $$d(x,y,2) = (\\sum_{i=1}^{n}{|x_i - y_i|^2})^{1/2}$$\n",
    "\n",
    "#### $$d(x,y,\\infty) = \\max(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vamos criar uma função pNorm que recebe como parâmetro p e retorna uma função que calcula a pNorma\n",
    "def pNorm(p):\n",
    "    \"\"\"Generates a function to calculate the p-Norm between two points.\n",
    "\n",
    "    Args:\n",
    "        p (int): The integer p.\n",
    "\n",
    "    Returns:\n",
    "        Dist: A function that calculates the p-Norm.\n",
    "    \"\"\"\n",
    "\n",
    "    def Dist(x,y):\n",
    "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
    "    return Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores numéricos\n",
    "numPointsRDD = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 4.76 3.6\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartPointsRDD = numPointsRDD.cartesian(numPointsRDD)\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "\n",
    "#print cartPointsRDD.take(1)\n",
    "cartPointsParesRDD = cartPointsRDD.map(lambda rec: ((rec[0][0],rec[1][0]), (rec[0][1],rec[1][1]))) #recarray\n",
    "\n",
    "\n",
    "\n",
    "# Aplique um mapa para calcular a Distância Euclidiana entre os pares\n",
    "Euclid = pNorm(2)\n",
    "distRDD = cartPointsParesRDD.map(lambda rec: (rec[0], Euclid(rec[1][0],rec[1][1])))\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statRDD = distRDD.map(lambda rec: rec[1])\n",
    "\n",
    "minv, maxv, meanv = statRDD.min(), statRDD.max(), statRDD.mean()\n",
    "minv, maxv, meanv = minv.round(2), maxv.round(2), meanv.round(2)\n",
    "print minv, maxv, meanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Valores incorretos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-400e44f76b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mminv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeanv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Valores incorretos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Valores incorretos"
     ]
    }
   ],
   "source": [
    "assert (minv.round(2), maxv.round(2), meanv.round(2))==(0.0, 4.70, 3.65), 'Valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5b) Valores Categóricos **\n",
    "\n",
    "#### Quando nossos objetos são representados por atributos categóricos, eles não possuem uma similaridade espacial. Para calcularmos a similaridade entre eles podemos primeiro transformar nosso vetor de atrbutos em um vetor binário indicando, para cada possível valor de cada atributo, se ele possui esse atributo ou não.\n",
    "\n",
    "#### Com o vetor binário podemos utilizar a distância de Hamming  definida por:\n",
    "\n",
    "#### $$ H(x,y) = \\sum_{i=1}^{n}{x_i != y_i} $$\n",
    "\n",
    "#### Também é possível definir a distância de Jaccard como:\n",
    "\n",
    "#### $$ J(x,y) = \\frac{\\sum_{i=1}^{n}{x_i == y_i} }{\\sum_{i=1}^{n}{\\max(x_i, y_i}) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar uma função para calcular a distância de Hamming\n",
    "def Hamming(x,y):\n",
    "    \"\"\"Calculates the Hamming distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        H (int): The Hamming distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x!=y).sum()\n",
    "\n",
    "# Vamos criar uma função para calcular a distância de Jaccard\n",
    "def Jaccard(x,y):\n",
    "    \"\"\"Calculates the Jaccard distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        J (int): The Jaccard distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x==y).sum()/float( np.maximum(x,y).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores categóricos\n",
    "catPointsRDD = sc.parallelize(enumerate([['alto', 'caro', 'azul'],\n",
    "                             ['medio', 'caro', 'verde'],\n",
    "                             ['alto', 'barato', 'azul'],\n",
    "                             ['medio', 'caro', 'vermelho'],\n",
    "                             ['baixo', 'barato', 'verde'],\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alto': 0, 'medio': 7, 'baixo': 5, 'barato': 2, 'azul': 4, 'verde': 6, 'caro': 3, 'vermelho': 1} 8\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Crie um RDD de chaves únicas utilizando flatMap\n",
    "\n",
    "#print catPointsRDD.collect()\n",
    "chavesRDD = (catPointsRDD\n",
    "             .flatMap(lambda rec: map(lambda x: (x,1),rec[1]))\n",
    "             .reduceByKey(lambda x,y: x)\n",
    "             .map(lambda rec: rec[0])\n",
    "             )\n",
    "\n",
    "#print chavesRDD.collect()\n",
    "\n",
    "chaves = dict((v,k) for k,v in enumerate(chavesRDD.collect())) #chaves fora de ordem\n",
    "nchaves = len(chaves)\n",
    "print chaves, nchaves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valores incorretos!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3eeb8e1f9804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mchaves\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'alto'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medio'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'baixo'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'barato'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'azul'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'verde'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'caro'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vermelho'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valores incorretos!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnchaves\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'número de chaves incorreta'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valores incorretos!"
     ]
    }
   ],
   "source": [
    "assert chaves=={'alto': 0, 'medio': 1, 'baixo': 2, 'barato': 3, 'azul': 4, 'verde': 5, 'caro': 6, 'vermelho': 7}, 'valores incorretos!'\n",
    "print \"OK\"\n",
    "\n",
    "assert nchaves==8, 'número de chaves incorreta'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.])),\n",
       " (1, array([ 0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.])),\n",
       " (2, array([ 1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.])),\n",
       " (3, array([ 0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.])),\n",
       " (4, array([ 0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.]))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CreateNP(atributos,chaves):  \n",
    "    \"\"\"Binarize the categorical vector using a dictionary of keys.\n",
    "\n",
    "    Args:\n",
    "        atributos (list): List of attributes of a given object.\n",
    "        chaves (dict): dictionary with the relation attribute -> index\n",
    "\n",
    "    Returns:\n",
    "        array (np.array): Binary array of attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.zeros(len(chaves))\n",
    "    for atr in atributos:\n",
    "        array[ chaves[atr] ] = 1\n",
    "    return array\n",
    "\n",
    "# Converte o RDD para o formato binário, utilizando o dict chaves\n",
    "binRDD = catPointsRDD.map(lambda rec: (rec[0],CreateNP(rec[1], chaves)))\n",
    "binRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMin\tMax\tMean\n",
      "Hamming:\t0.00\t6.00\t3.52\n",
      "Jaccard:\t0.33\t2.67\t1.14\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartBinRDD = binRDD.cartesian(binRDD)\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "cartBinParesRDD = cartBinRDD.map(lambda rec: ((rec[0][0],rec[1][0]),(rec[0][1],rec[1][1])))\n",
    "\n",
    "\n",
    "# Aplique um mapa para calcular a Distância de Hamming e Jaccard entre os pares\n",
    "hamRDD = cartBinParesRDD.map(lambda rec: (rec[0], Hamming(rec[1][0],rec[1][1])))\n",
    "jacRDD = cartBinParesRDD.map(lambda rec: (rec[0], Jaccard(rec[1][0],rec[1][1])))\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statHRDD = hamRDD.map(lambda rec: rec[1])\n",
    "statJRDD = jacRDD.map(lambda rec: rec[1])\n",
    "\n",
    "Hmin, Hmax, Hmean = statHRDD.min(), statHRDD.max(), statHRDD.mean()\n",
    "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
    "\n",
    "print \"\\t\\tMin\\tMax\\tMean\"\n",
    "print \"Hamming:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(Hmin, Hmax, Hmean )\n",
    "print \"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert (Hmin.round(2), Hmax.round(2), Hmean.round(2)) == (0.00,6.00,3.52), 'valores incorretos'\n",
    "print \"OK\"\n",
    "assert (Jmin.round(2), Jmax.round(2), Jmean.round(2)) == (0.33,2.67,1.14), 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
